# v1.5 "Live Intelligence" Design

**Status:** Draft
**Author:** Claude + User
**Date:** 2026-01-09

## Overview

v1.5 extends the historical intelligence system (v1.0) with live regime awareness. The core v1.0 contract remains inviolate: deterministic regime classification, retrieval of similar historical regimes, parameter recommendations with explainable justification, and OOS performance transparency.

**Version semantics:**
- v1.0 = Historical Intelligence (batch, retrospective)
- v1.5 = Live Regime Awareness (streaming, accountable)
- v2.0 = Autonomous Adaptation (future)

**Key invariant:** All v1.5 features reference v1.0 artifacts—they do not replace them.

---

## Invariants (v1.5 Contract)

### 1. Dimensional Taxonomy

`regime_key` is always a canonical composite of fixed dimensions. No free-form tags in v1.5.

**Dimensions (v1.5):**
- `trend`: `uptrend` | `downtrend` | `flat`
- `vol`: `low_vol` | `mid_vol` | `high_vol`

**Canonical key format:**
```
trend=uptrend|vol=high_vol
```

Rules:
- Fixed dimension order (alphabetical)
- Fixed value normalization
- Join with `|`

This bounds combinatorics: 3 × 3 = 9 composite keys per timeframe.

### 2. Window Declaration

All fields that depend on rolling data must include explicit window metadata. This prevents "what does this number represent?" ambiguity.

```json
{
  "windows": {
    "regime_age_bars": 120,
    "performance_window": {"bars": 500, "timeframe": "5m"},
    "distance_window": {"bars": 500, "timeframe": "5m"}
  }
}
```

---

## Primitive A: Explicit Confidence

Confidence is decomposed into orthogonal, auditable components.

### Fields

| Field | Description | Required |
|-------|-------------|----------|
| `regime_fit_confidence` | How well current market matches historical regime clusters (0-1) | Always |
| `recommendation_confidence` | Quality of parameter aggregation (0-1) | Always |
| `historical_support` | Number of trials backing this recommendation | Always |
| `regime_distance_z` | Z-score: how far current features are from the retrieved neighborhood, normalized | Always |
| `performance_surprise_z` | Z-score: realized vs expected performance | Conditional |

### API Payload

```json
{
  "regime_fit_confidence": 0.81,
  "recommendation_confidence": 0.72,
  "historical_support": 14,
  "regime_distance_z": 0.6,
  "performance_surprise_z": -0.9,

  "distance_baseline": "composite",
  "distance_n": 47,

  "performance_baseline": "matched_trials",
  "performance_support_n": 14,

  "windows": {
    "regime_age_bars": 120,
    "performance_window": {"bars": 500, "timeframe": "5m"},
    "distance_window": {"bars": 500, "timeframe": "5m"}
  },

  "missing": []
}
```

When `performance_surprise_z` is unavailable:
```json
{
  "performance_surprise_z": null,
  "missing": ["no_forward_run_metrics"]
}
```

### regime_distance_z Computation

**Definition:** How far current features are from the retrieved neighborhood, normalized.

- **Cluster stats provide feature scaling** (diagonal variances for standardization)
- **Neighbors provide local baseline distribution** (what "typical" distance looks like)

**Distance metric:** Standardized Euclidean (diagonal Mahalanobis)

```
d(x, y) = sqrt(sum_i ((x_i - y_i)^2 / (sigma_i^2 + epsilon)))
```

Where `sigma_i^2` comes from ingest-time diagonal variance for that cluster (feature scaling).

**Algorithm:**
1. Compute distances from current feature vector to top-K neighbors
2. Build neighbor distance distribution: `mu = median(D)`, `sigma_obs = 1.4826 * MAD(D)`
3. Apply shrinkage (blend cluster prior with observed): `sigma = alpha * sigma_prior + (1 - alpha) * sigma_obs`
   - `alpha = c / (c + K)` where c ~ 10-30
   - `sigma_prior` from cluster stats
4. Compute z-score: `z = (d_now - mu) / (sigma + epsilon)`

This keeps cluster stats for scaling while neighbors define what "normal" distance looks like for this query.

### performance_surprise_z Computation

```
z = (metric_recent - mu_expected) / (sigma_expected + epsilon)
```

**Inputs:**
- Live stream: rolling window of realized outcomes (returns, Sharpe proxy, hit rate, drawdown)
- KB: expected distribution (mean, std) from matched historical trials

**Interpretation:**
- `z = -0.9` → 0.9 std below expectation (notable)
- `z <= -2.0` → statistically meaningful underperformance (alert territory)

**Requirements:**
- Forward run active (paper/shadow/live)
- Minimum observation window (30-100 trades or N bars)

### Storage: regime_cluster_stats

```sql
CREATE TABLE regime_cluster_stats (
    strategy_entity_id UUID NOT NULL,
    timeframe TEXT NOT NULL,
    regime_key TEXT NOT NULL,
    regime_dims JSONB NOT NULL,

    n INT NOT NULL,
    feature_schema_version INT NOT NULL DEFAULT 1,
    feature_mean JSONB NOT NULL,
    feature_var JSONB NOT NULL,
    feature_min JSONB,
    feature_max JSONB,

    updated_at TIMESTAMPTZ DEFAULT now(),
    PRIMARY KEY (strategy_entity_id, timeframe, regime_key)
);

-- Index for backoff queries (marginals)
CREATE INDEX idx_cluster_stats_strategy_timeframe
    ON regime_cluster_stats(strategy_entity_id, timeframe);
```

**Backoff chain:**
1. Exact composite: `trend=uptrend|vol=high_vol`
2. Marginals: `trend=uptrend` or `vol=high_vol` (take max variance per feature)
3. Neighbor-only: use query-time top-K distribution, mark `"distance_baseline": "neighbors_only"`

---

## Primitive B: Expected vs Realized

Every recommendation creates a future accountability record.

### Entities

```
recommendation_record (contract, long-lived)
    ├── recommendation_observations (streaming aggregates, append-only)
    └── recommendation_evaluation_slices (checkpoints, immutable)
```

### recommendation_record

Created at recommend time. Stores immutable expectation contract.

```sql
CREATE TABLE recommendation_records (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workspace_id UUID NOT NULL,
    strategy_entity_id UUID NOT NULL,
    symbol TEXT NOT NULL,
    timeframe TEXT NOT NULL,

    params_json JSONB NOT NULL,
    params_hash TEXT NOT NULL,

    regime_key_start TEXT NOT NULL,
    regime_dims_start JSONB NOT NULL,
    regime_features_start JSONB NOT NULL,

    schema_version INT NOT NULL DEFAULT 1,
    confidence_json JSONB NOT NULL,
    expected_baselines_json JSONB NOT NULL,

    status TEXT NOT NULL DEFAULT 'active',
    -- Status definitions:
    -- active: currently running recommendation
    -- superseded: replaced by newer recommendation for same symbol+strategy
    -- inactive: no forward data received for TTL days (auto-transition)
    -- closed: manually ended by user or end of experiment

    created_at TIMESTAMPTZ DEFAULT now(),
    updated_at TIMESTAMPTZ DEFAULT now()
);

-- Only one active recommendation per symbol+strategy+workspace
CREATE UNIQUE INDEX idx_records_active_unique
    ON recommendation_records(workspace_id, strategy_entity_id, symbol, timeframe)
    WHERE status = 'active';

-- Query by workspace and status
CREATE INDEX idx_records_workspace_status
    ON recommendation_records(workspace_id, status);
```

### recommendation_observations

Append-only streaming aggregates. Store time-bucket aggregates, not raw events.

```sql
CREATE TABLE recommendation_observations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    record_id UUID NOT NULL REFERENCES recommendation_records(id),
    ts TIMESTAMPTZ NOT NULL,

    bars_seen INT NOT NULL,
    trades_seen INT NOT NULL,
    realized_metrics_json JSONB NOT NULL,

    created_at TIMESTAMPTZ DEFAULT now(),

    -- Idempotency: one observation per record per timestamp
    UNIQUE (record_id, ts)
);

-- Query recent observations for a record
CREATE INDEX idx_observations_record_ts
    ON recommendation_observations(record_id, ts DESC);
```

### recommendation_evaluation_slices

Auto-generated snapshots at natural boundaries.

```sql
CREATE TABLE recommendation_evaluation_slices (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    record_id UUID NOT NULL REFERENCES recommendation_records(id),

    slice_start_ts TIMESTAMPTZ NOT NULL,
    slice_end_ts TIMESTAMPTZ NOT NULL,

    trigger_type TEXT NOT NULL,
    -- regime_change | milestone | manual

    regime_key_during TEXT NOT NULL,
    realized_summary_json JSONB NOT NULL,
    expected_summary_json JSONB NOT NULL,
    performance_surprise_z FLOAT,
    drift_flags_json JSONB,

    created_at TIMESTAMPTZ DEFAULT now()
);

-- Query slices by record (most recent first)
CREATE INDEX idx_slices_record_end
    ON recommendation_evaluation_slices(record_id, slice_end_ts DESC);

-- Query by trigger type (for analytics)
CREATE INDEX idx_slices_trigger_type
    ON recommendation_evaluation_slices(trigger_type);
```

### Slice Triggers

| Trigger | Condition |
|---------|-----------|
| `regime_change` | Stable regime state changed (hysteresis FSM confirmed) |
| `milestone` | Every N trades OR every Δt |
| `manual` | User snapshot request |

### Record Close Triggers

| Trigger | Condition |
|---------|-----------|
| `superseded` | New recommendation activated for same strategy+symbol |
| `inactive` | No forward data for TTL days |
| `manual` | Optional user action |

**Note:** Closure is operational, not analytical. Slices are snapshots, not closes.

### Stability Guard: Hysteresis FSM

Prevents slice spam from regime flicker.

**Parameters:**
- `M` = persistence bars (default: 20)
- `C_enter` = enter threshold (default: 0.75)
- `C_exit` = exit threshold (default: 0.55), where `C_exit < C_enter`

**Configuration:** FSM parameters are configurable per workspace + timeframe (not per request). Store in `workspace_config` or `strategy_config` JSONB. Hard defaults apply when not specified.

**State:**
- `stable_regime_key` — confirmed regime
- `candidate_regime_key` — potential new regime
- `candidate_count` — bars candidate has persisted

**Algorithm:**

```python
def update_regime_state(raw_regime_key, raw_confidence):
    if raw_regime_key == stable_regime_key:
        # Still in same regime
        candidate_regime_key = None
        candidate_count = 0
        return

    if raw_confidence < C_exit:
        # Too weak, ignore as noise
        return

    if raw_regime_key != candidate_regime_key:
        # New candidate
        candidate_regime_key = raw_regime_key
        candidate_count = 1
    else:
        # Same candidate, increment
        candidate_count += 1

    # Check confirmation
    if candidate_count >= M:
        median_conf = median(confidence_buffer[-M:])
        if median_conf >= C_enter:
            # Confirmed transition
            emit_regime_change_event()
            create_evaluation_slice()
            stable_regime_key = candidate_regime_key
            candidate_regime_key = None
            candidate_count = 0
```

**API exposure:**

```json
{
  "stable_regime_key": "trend=uptrend|vol=high_vol",
  "raw_regime_key": "trend=flat|vol=high_vol",

  "regime_fit_confidence": 0.78,
  "regime_state_stability": {
    "candidate_key": "trend=flat|vol=high_vol",
    "candidate_bars": 12,
    "M": 20,
    "C_enter": 0.75,
    "C_exit": 0.55
  }
}
```

---

## Primitive C: Regime Duration

Regime duration is as important as regime type.

### Fields

| Field | Description |
|-------|-------------|
| `regime_age_bars` | Bars since stable regime confirmed |
| `regime_half_life_bars` | Median historical duration for this regime key |
| `expected_remaining_bars` | max(0, median - age) |
| `duration_iqr_bars` | [p25, p75] historical duration |
| `remaining_iqr_bars` | [max(0, p25-age), max(0, p75-age)] |

### API Payload

```json
{
  "regime_age_bars": 120,

  "regime_half_life_bars": 240,
  "expected_remaining_bars": 120,

  "duration_iqr_bars": [180, 310],
  "remaining_iqr_bars": [60, 190],

  "duration_baseline": "composite_symbol",
  "duration_n": 63,

  "missing": []
}
```

With backoff:
```json
{
  "duration_baseline": "trend_marginal_symbol",
  "duration_n": 18
}
```

Unavailable:
```json
{
  "regime_half_life_bars": null,
  "expected_remaining_bars": null,
  "missing": ["no_duration_stats"]
}
```

### Estimation Method

**half_life** = median historical duration for composite regime key

**Approach:**
1. Run hysteresis FSM on historical OHLCV to segment into stable regimes
2. Each segment yields: `regime_key`, `start_ts`, `end_ts`, `duration_bars`
3. Aggregate by regime_key: median, p25, p75

### Storage: regime_duration_stats

Keyed by market properties, not strategy (duration is market behavior).

```sql
CREATE TABLE regime_duration_stats (
    symbol TEXT NOT NULL,
    timeframe TEXT NOT NULL,
    regime_key TEXT NOT NULL,

    n_segments INT NOT NULL,
    median_duration_bars INT NOT NULL,
    p25_duration_bars INT NOT NULL,
    p75_duration_bars INT NOT NULL,

    updated_at TIMESTAMPTZ DEFAULT now(),
    PRIMARY KEY (symbol, timeframe, regime_key)
);

-- Index for backoff queries (marginals, global timeframe)
CREATE INDEX idx_duration_stats_timeframe_key
    ON regime_duration_stats(timeframe, regime_key);
```

**Backoff chain:**
1. `composite_symbol` — exact key for symbol+timeframe
2. `marginal_symbol` — single dimension (trend-only or vol-only)
3. `global_timeframe` — any regime for this timeframe
4. `null` + `missing: ["no_duration_stats"]`

---

## v1.5 Scope Summary

### New Tables

| Table | Purpose |
|-------|---------|
| `regime_cluster_stats` | Feature centroids + variances per regime key |
| `regime_duration_stats` | Duration distributions per regime key |
| `recommendation_records` | Expectation contracts |
| `recommendation_observations` | Streaming realized metrics |
| `recommendation_evaluation_slices` | Accountability checkpoints |

### New Capabilities

| Capability | Status |
|------------|--------|
| Live regime classification | v1.5 |
| Regime confidence & age tracking | v1.5 |
| Parameter drift detection (regime_distance_z) | v1.5 |
| Expected vs realized tracking | v1.5 |
| Regime persistence estimation (median duration + IQR) | v1.5 |
| Human-readable alerts | v1.5 |
| performance_surprise_z | v1.5 (conditional on forward run) |

### Philosophical Choice

The system never acts unilaterally in v1.5. It detects, alerts, and recommends. User decides.

---

## User Loop (v1.5)

```
1. User runs strategy
2. System tracks live regime (stable + raw)
3. System monitors expected vs realized
4. Drift detected → alert (slice created)
5. Recommendation updated (new params available)
6. User decides
```

---

## Forward Metrics Interface (Minimal Contract)

For `performance_surprise_z` to work, the system needs an interface to receive realized metrics.

```
POST /forward/metrics

Request:
{
    "workspace_id": "uuid",
    "record_id": "uuid",
    "ts": "2026-01-09T12:00:00Z",
    "bars_seen": 500,
    "trades_seen": 42,
    "realized_metrics": {
        "return_pct": 0.023,
        "sharpe_proxy": 1.2,
        "hit_rate": 0.58,
        "max_drawdown_pct": 0.08,
        "expectancy": 0.15
    }
}

Response:
{
    "status": "accepted",
    "observation_id": "uuid"
}
```

**Guarantees:**
- Idempotency via `(record_id, ts)` unique constraint
- Returns 409 Conflict if duplicate
- Returns 404 if record_id not found or not active

Even if stubbed initially, having this contract prevents design drift.

---

## Backfill Requirements

### regime_cluster_stats
- Computed from existing ingested trials
- Each trial already has `regime_features` in payload
- Background job aggregates by (strategy_entity_id, timeframe, regime_key)

### regime_duration_stats
- **Requires historical OHLCV segmentation**
- Run the same hysteresis FSM over historical OHLCV per symbol/timeframe
- Generate stable regime segments, then aggregate durations by regime_key
- This is a batch job, not derivable from existing trials alone

---

## Open Questions (for implementation planning)

1. **Historical OHLCV source** — Where does the backfill job get historical OHLCV for duration stats? (Existing datasets? External API?)
2. **Alert delivery** — How are regime change / drift alerts surfaced to users? (Webhook? Email? In-app?)
3. **Forward run sources** — What systems will call POST /forward/metrics? (Paper trading engine? Shadow execution? Manual upload?)

---

## Next Steps

1. Review and approve this design
2. Create implementation plan with task breakdown
3. Set up git worktree for v1.5 feature branch
